# Evaluation of Agentic AI Systems

Evaluating agents is fundamentally different from evaluating static LLM output.

---

### 1. Evaluation Dimensions

- factual correctness  
- grounding  
- safety  
- action correctness  
- tool performance  
- latency  
- plan efficiency  
- stability across runs  

---

### 2. Benchmarks

- GAIA  
- SWE-bench  
- AgentBench  
- ToolBench  
- LongBench  
- HumanEval (for coding agents)

---

### 3. Evaluation Types

### **Static Evaluation**
Predefined question-answer tests.

### **Dynamic Evaluation**
Interactive or multi-step tasks.

### **Simulated Environments**
Sandboxes for safe tool testing.

### **Human Evaluation**
Used for quality, style, or risk-sensitive tasks.

---

### Summary
Evaluation ensures agents remain reliable, measurable, and production-ready.
